# AUTOGENERATED! DO NOT EDIT! File to edit: ../04_mlp-learning.ipynb.

# %% auto 0
__all__ = ['sigmoid', 'derivative_sigmoid', 'train_mlp']

# %% ../04_mlp-learning.ipynb 5
import numpy as np
import matplotlib.pyplot as plt

# %% ../04_mlp-learning.ipynb 9
# Sigmoid Function
def sigmoid(x):
    ### BEGIN SOLUTION
    return 1 / (1 + np.exp(-x))
    ### END SOLUTION

# %% ../04_mlp-learning.ipynb 13
# Derivative of Sigmoid Function
def derivative_sigmoid(x):
    return x * (1 - x)

# %% ../04_mlp-learning.ipynb 17
def train_mlp(d_in, d_h, d_out, epoch = 10000, learning_rate = 0.1):

    # weight and bias initialization
    wh = np.random.uniform(size=(d_in, d_h))
    bh = np.random.uniform(size=(1, d_h))
    wout = np.random.uniform(size=(d_h, d_out))
    bout = np.random.uniform(size=(1, d_out))
    loss_epoch = []

    for i in range(epoch):
        # Forward pass
        h = sigmoid(X.dot(wh) + bh)
        y_pred = sigmoid(h.dot(wout) + bout)

        # Compute and print loss
        loss = (y_pred - y).sum()
        loss_epoch.append(loss)

        if i % 500 == 0:
            print('Epoch', i, ':', loss)

        # Backpropagation to compute gradients
        grad_y_pred = (y - y_pred) * derivative_sigmoid(y_pred)
        grad_wout = h.T.dot(grad_y_pred)
        grad_bout = np.sum(grad_y_pred, axis=0, keepdims=True)
        grad_h = grad_y_pred.dot(wout.T) * derivative_sigmoid(h)
        grad_wh = X.T.dot(grad_h)
        grad_bh = np.sum(grad_h, axis=0, keepdims=True)

        # Update weights and biases
        # hint:
        # you should update wout, bout, wh, bh with grad_wout, grad_bout, grad_wh, grad_bh multiplied by learning_rate
        ### BEGIN SOLUTION
        wout += grad_wout * learning_rate
        bout += grad_bout * learning_rate
        wh += grad_wh * learning_rate
        bh += grad_bh * learning_rate
        ### END SOLUTION
    
    return loss_epoch, loss, y_pred

