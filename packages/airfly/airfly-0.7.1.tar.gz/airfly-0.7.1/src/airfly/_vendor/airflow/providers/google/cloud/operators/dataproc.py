# Auto generated by 'inv collect-airflow'
from airfly._vendor.airflow.models.baseoperator import BaseOperator


class DataprocCreateClusterOperator(BaseOperator):
    cluster_name: "str"
    region: "typing.Union[str, NoneType]"
    project_id: "typing.Union[str, NoneType]"
    cluster_config: "typing.Union[typing.Dict, NoneType]"
    labels: "typing.Union[typing.Dict, NoneType]"
    request_id: "typing.Union[str, NoneType]"
    delete_on_error: "bool"
    use_if_exists: "bool"
    retry: "typing.Union[google.api_core.retry.Retry, NoneType]"
    timeout: "float"
    metadata: "typing.Union[typing.Sequence[typing.Tuple[str, str]], NoneType]"
    gcp_conn_id: "str"
    impersonation_chain: "typing.Union[str, typing.Sequence[str], NoneType]"


class DataprocDeleteClusterOperator(BaseOperator):
    project_id: "str"
    region: "str"
    cluster_name: "str"
    cluster_uuid: "typing.Union[str, NoneType]"
    request_id: "typing.Union[str, NoneType]"
    retry: "typing.Union[google.api_core.retry.Retry, NoneType]"
    timeout: "typing.Union[float, NoneType]"
    metadata: "typing.Union[typing.Sequence[typing.Tuple[str, str]], NoneType]"
    gcp_conn_id: "str"
    impersonation_chain: "typing.Union[str, typing.Sequence[str], NoneType]"


class DataprocInstantiateInlineWorkflowTemplateOperator(BaseOperator):
    template: "typing.Dict"
    region: "str"
    project_id: "typing.Union[str, NoneType]"
    request_id: "typing.Union[str, NoneType]"
    retry: "typing.Union[google.api_core.retry.Retry, NoneType]"
    timeout: "typing.Union[float, NoneType]"
    metadata: "typing.Union[typing.Sequence[typing.Tuple[str, str]], NoneType]"
    gcp_conn_id: "str"
    impersonation_chain: "typing.Union[str, typing.Sequence[str], NoneType]"


class DataprocInstantiateWorkflowTemplateOperator(BaseOperator):
    template_id: "str"
    region: "str"
    project_id: "typing.Union[str, NoneType]"
    version: "typing.Union[int, NoneType]"
    request_id: "typing.Union[str, NoneType]"
    parameters: "typing.Union[typing.Dict[str, str], NoneType]"
    retry: "typing.Union[google.api_core.retry.Retry, NoneType]"
    timeout: "typing.Union[float, NoneType]"
    metadata: "typing.Union[typing.Sequence[typing.Tuple[str, str]], NoneType]"
    gcp_conn_id: "str"
    impersonation_chain: "typing.Union[str, typing.Sequence[str], NoneType]"


class DataprocJobBaseOperator(BaseOperator):
    job_name: "str"
    cluster_name: "str"
    project_id: "typing.Union[str, NoneType]"
    dataproc_properties: "typing.Union[typing.Dict, NoneType]"
    dataproc_jars: "typing.Union[typing.List[str], NoneType]"
    gcp_conn_id: "str"
    delegate_to: "typing.Union[str, NoneType]"
    labels: "typing.Union[typing.Dict, NoneType]"
    region: "typing.Union[str, NoneType]"
    job_error_states: "typing.Union[typing.Set[str], NoneType]"
    impersonation_chain: "typing.Union[str, typing.Sequence[str], NoneType]"
    asynchronous: "bool"


class DataprocScaleClusterOperator(BaseOperator):
    cluster_name: "str"
    project_id: "typing.Union[str, NoneType]"
    region: "str"
    num_workers: "int"
    num_preemptible_workers: "int"
    graceful_decommission_timeout: "typing.Union[str, NoneType]"
    gcp_conn_id: "str"
    impersonation_chain: "typing.Union[str, typing.Sequence[str], NoneType]"


class DataprocSubmitHadoopJobOperator(DataprocJobBaseOperator):
    main_jar: "typing.Union[str, NoneType]"
    main_class: "typing.Union[str, NoneType]"
    arguments: "typing.Union[typing.List, NoneType]"
    archives: "typing.Union[typing.List, NoneType]"
    files: "typing.Union[typing.List, NoneType]"


class DataprocSubmitHiveJobOperator(DataprocJobBaseOperator):
    query: "typing.Union[str, NoneType]"
    query_uri: "typing.Union[str, NoneType]"
    variables: "typing.Union[typing.Dict, NoneType]"


class DataprocSubmitPigJobOperator(DataprocJobBaseOperator):
    query: "typing.Union[str, NoneType]"
    query_uri: "typing.Union[str, NoneType]"
    variables: "typing.Union[typing.Dict, NoneType]"


class DataprocSubmitPySparkJobOperator(DataprocJobBaseOperator):
    main: "str"
    arguments: "typing.Union[typing.List, NoneType]"
    archives: "typing.Union[typing.List, NoneType]"
    pyfiles: "typing.Union[typing.List, NoneType]"
    files: "typing.Union[typing.List, NoneType]"


class DataprocSubmitSparkJobOperator(DataprocJobBaseOperator):
    main_jar: "typing.Union[str, NoneType]"
    main_class: "typing.Union[str, NoneType]"
    arguments: "typing.Union[typing.List, NoneType]"
    archives: "typing.Union[typing.List, NoneType]"
    files: "typing.Union[typing.List, NoneType]"


class DataprocSubmitSparkSqlJobOperator(DataprocJobBaseOperator):
    query: "typing.Union[str, NoneType]"
    query_uri: "typing.Union[str, NoneType]"
    variables: "typing.Union[typing.Dict, NoneType]"
