# Auto generated by 'inv collect-airflow'
from airfly._vendor.airflow.models.baseoperator import BaseOperator


class DatabricksRunNowOperator(BaseOperator):
    job_id: "typing.Union[str, NoneType]"
    json: "typing.Union[typing.Any, NoneType]"
    notebook_params: "typing.Union[typing.Dict[str, str], NoneType]"
    python_params: "typing.Union[typing.List[str], NoneType]"
    spark_submit_params: "typing.Union[typing.List[str], NoneType]"
    databricks_conn_id: "str"
    polling_period_seconds: "int"
    databricks_retry_limit: "int"
    databricks_retry_delay: "int"
    do_xcom_push: "bool"


class DatabricksSubmitRunOperator(BaseOperator):
    json: "typing.Union[typing.Any, NoneType]"
    spark_jar_task: "typing.Union[typing.Dict[str, str], NoneType]"
    notebook_task: "typing.Union[typing.Dict[str, str], NoneType]"
    spark_python_task: "typing.Union[typing.Dict[str, typing.Union[str, typing.List[str]]], NoneType]"
    spark_submit_task: "typing.Union[typing.Dict[str, typing.List[str]], NoneType]"
    new_cluster: "typing.Union[typing.Dict[str, object], NoneType]"
    existing_cluster_id: "typing.Union[str, NoneType]"
    libraries: "typing.Union[typing.List[typing.Dict[str, str]], NoneType]"
    run_name: "typing.Union[str, NoneType]"
    timeout_seconds: "typing.Union[int, NoneType]"
    databricks_conn_id: "str"
    polling_period_seconds: "int"
    databricks_retry_limit: "int"
    databricks_retry_delay: "int"
    do_xcom_push: "bool"
